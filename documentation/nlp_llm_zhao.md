# Notes all about NLP and LLM from Udemy course from Alice Zhao

## History of NLP

### Brief History of NLP (Natural Language Processing) for Study Notes

- **1950s: Foundations**
  - Alan Turing proposes the Turing Test (1950) to evaluate machine intelligence via language.
  - Early work focuses on rule-based systems and machine translation (e.g., Georgetown-IBM experiment, 1954).

- **1960s-1980s: Rule-Based Systems**
  - Development of symbolic NLP with hand-crafted rules and grammars.
  - Systems like ELIZA (1966) simulate conversation using pattern matching.
  - SHRDLU (1968-1970) demonstrates language understanding in a block world.
  - Limited by rigid rules and lack of scalability.

- **1980s-1990s: Statistical NLP**
  - Shift to data-driven approaches with statistical models.
  - Introduction of Hidden Markov Models (HMMs) and n-grams for tasks like speech recognition and part-of-speech tagging.
  - IBM’s statistical machine translation models (e.g., Candide, 1988) gain traction.
  - Rise of corpora (e.g., Brown Corpus) enables training on real-world text.

- **2000s: Machine Learning Boom**
  - Adoption of machine learning algorithms (e.g., SVMs, decision trees) for NLP tasks.
  - WordNet (1985, expanded in 2000s) provides lexical resources for semantic analysis.
  - Advances in information retrieval, sentiment analysis, and named entity recognition.
  - Early neural network applications (e.g., word embeddings like Word2Vec, 2013).

- **2010s: Deep Learning Revolution**
  - Deep learning transforms NLP with neural networks (RNNs, LSTMs, CNNs).
  - Word embeddings (Word2Vec, GloVe) capture semantic relationships.
  - Sequence-to-sequence models (2014) improve machine translation.
  - Introduction of the Transformer model (2017, Vaswani et al.) with attention mechanisms revolutionizes NLP.
  - BERT (2018) and other pre-trained models enable transfer learning for diverse tasks.

- **2020s: Large Language Models (LLMs)**
  - Explosion of LLMs like GPT-3 (2020), T5, and LLaMA, trained on massive datasets.
  - Focus on few-shot/zero-shot learning, multimodal NLP (text + images), and ethical considerations.
  - Applications expand to chatbots, content generation, and real-time translation.
  - Challenges include bias, computational costs, and interpretability.

**Key Trends**: Rule-based → Statistical → Neural → Transformer-based models. Ongoing focus on scalability, efficiency, and responsible AI.

**Note**: For deeper study, explore primary papers (e.g., "Attention is All You Need") and datasets like SQuAD, GLUE, or Common Crawl.

